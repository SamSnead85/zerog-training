# Unit 2: Inside the Black Box - How LLMs Work

> **Unit Duration**: 5 hours | **Lessons**: 6 + Lab
> **Module**: 1 - AI Fundamentals for Modern Development

---

## Unit Overview

This is the most technically important unit in the curriculum. Understanding how large language models actually work—not just conceptually, but mechanistically—separates AI practitioners from AI consumers. You'll learn about transformers, attention, embeddings, and the complete training pipeline from pre-training through alignment.

---

## Learning Objectives

By completing this unit, you will be able to:

1. Explain the transformer architecture and why it revolutionized NLP
2. Describe how self-attention allows models to capture long-range dependencies
3. Understand embeddings as learned representations in high-dimensional space
4. Break down text into tokens and calculate token counts
5. Explain the pre-training objective and why scale matters
6. Compare alignment techniques: SFT, RLHF, and DPO

---

## Lesson List

| Lesson | Topic | Duration | Key Concepts |
|--------|-------|----------|--------------|
| 2.1 | The Transformer Architecture | 45 min | Encoder-decoder, attention layers |
| 2.2 | Attention is All You Need | 50 min | Self-attention, multi-head attention |
| 2.3 | Embeddings and Vector Representations | 40 min | Word vectors, semantic similarity |
| 2.4 | Tokenization: How Models See Text | 35 min | BPE, SentencePiece, token counting |
| 2.5 | Pre-Training at Scale | 45 min | Next-token prediction, emergent abilities |
| 2.6 | Alignment: SFT, RLHF, and DPO | 50 min | Making models helpful and safe |

---

## Key Papers

1. **"Attention Is All You Need"** (Vaswani et al., 2017) - The foundational transformer paper
2. **"BERT: Pre-training of Deep Bidirectional Transformers"** (Devlin et al., 2018)
3. **"Language Models are Few-Shot Learners"** (Brown et al., 2020) - GPT-3 paper
4. **"Training language models to follow instructions with human feedback"** (Ouyang et al., 2022) - RLHF/InstructGPT
5. **"Direct Preference Optimization"** (Rafailov et al., 2023) - Simplified alignment

---

## Prerequisites Check

Before starting this unit, ensure you can:

- [ ] Explain what a neural network does at a high level
- [ ] Understand vectors and basic linear algebra (dot products)
- [ ] Run Python code and install packages
- [ ] Navigate API documentation

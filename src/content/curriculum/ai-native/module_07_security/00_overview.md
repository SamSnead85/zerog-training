# Module 7: AI Security

> **Duration**: 10 hours | **Certification Path**: All Paths (Required)
> **Prerequisites**: Modules 1-6

---

## Module Overview

AI systems face unique security threats. This module covers prompt injection, data poisoning, adversarial attacks, and defense strategies.

---

## Learning Objectives

1. Understand LLM-specific attack vectors
2. Implement prompt injection defenses
3. Secure data pipelines against poisoning
4. Design defense-in-depth architectures
5. Build security monitoring and incident response

---

## Module Structure

### Unit 1: Threat Landscape (2 hours)
Understanding AI-specific security risks.

| Lesson | Topic | Duration |
|--------|-------|----------|
| 1.1 | LLM Attack Vectors | 45 min |
| 1.2 | OWASP LLM Top 10 | 50 min |
| 1.3 | Threat Modeling for AI | 55 min |

### Unit 2: Prompt Injection Defense (3 hours)
Protecting against the top LLM vulnerability.

| Lesson | Topic | Duration |
|--------|-------|----------|
| 2.1 | Injection Techniques | 55 min |
| 2.2 | Input Validation | 55 min |
| 2.3 | Output Sanitization | 50 min |
| 2.4 | Defense Patterns | 55 min |

### Unit 3: Data Security (3 hours)
Securing training and retrieval data.

| Lesson | Topic | Duration |
|--------|-------|----------|
| 3.1 | Data Poisoning | 55 min |
| 3.2 | PII Protection | 55 min |
| 3.3 | Access Control | 50 min |

### Unit 4: Security Operations (2 hours)
Monitoring and responding to threats.

| Lesson | Topic | Duration |
|--------|-------|----------|
| 4.1 | Security Monitoring | 55 min |
| 4.2 | Incident Response | 55 min |

---

## Capstone Project

**Secure LLM System**: Harden an LLM application with comprehensive security controls, monitoring, and incident response procedures.

---

## Key Resources

- OWASP LLM Top 10
- NIST AI Risk Management Framework
- Anthropic Constitutional AI
- Google Secure AI Framework
